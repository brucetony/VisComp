{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Sheet 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruce Schultz  \n",
    "bschultz@uni-bonn.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miguel A. Ibarra-Arellano  \n",
    "ibarrarellano@gmail.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part a\n",
    "\n",
    "**Pick the \"three clusters with equal numbers of points\" data set. Set the number of points per class to 10, and number of dimensions to 50. Once run the demo with perplexity=29, and once with perplexity=30. Explain why there is a big difference in the final 2D embedding?**\n",
    "* As explained in the article, perplexity is essentially a guess as to how many close neighbors each point has. Since there are a total of 30 points, by making the perplexity equal to 30, it causes the algorithm to fail. The perplexity, therefore, should always be set to a value that is less than the number of points. The first setting, with perplexity equal to 29, holds true to this rule and therefore is able to separate the values into distinct clusters\n",
    "\n",
    "**Try the example \"a square grid with equal spacing between points\", with 20 points per side. In the resulting plot with perplexity=100, why are distances between points in the middle of the square larger than near the boundary?**\n",
    "* t-SNE calculates the interaction of all points with every other point which is computationally expensive. One of the approximtions employed by this technique is to to reduce run time is to approximate the effect of clusters distant from the point. Here, distant clusters are calculated as a single value and their avaerage effect is then applied to the distant point. In this example, the densely packed corners of the shape are all pulling on the center points, so the center is being streched in several directions. The boundaries however have fewer directions in which they are interacting which is part of the reason they are able to become more packed. Ultimately it is the cumalative interaction of the distant points/clusters from all directions that make it so the points in the middle have greater distances between their neighbors compared to the points along the boundaries.\n",
    "\n",
    "**Pick \"a square grid with equal spacing between points\" data set, with 20 points per side, and perplexity=2. Run the t-SNE multiple times. You will observe that the square grid sometimes breaks down into separate smaller clusters. Why?**\n",
    "* As stated before, perplexity loosely defines the number of effective neighbors each dot should have. Additionally, it is known that if the perplexity is set too low for a large dataset, clusters may form when there should not be any due to small-scale variations that occur during the early iterations of calculations. This is also why when we run this simulation several times, it turns out different each time. Perplexity is also linked to the KL divergence that is calculated in that as perplexity increases, you will also get smalled KL-divergence values. With perplexity being so low in this simulation, KL-divergence values are high and false positive clusters form and separate. This shows why it is important to play with this parameter.\n",
    "\n",
    "**Use different perplexities for \"points randomly distributed in a circle\" with 100 points. Around what perplexity value does the resulting visualization start to resemble the input data set? Explain why the perplexity has to be large enough for the result to look like the input.**\n",
    "* When perplexity is set to a value of 10, the simulation begins to consistently generate a visualization that resembles the input data. As mentioned above, small perplexity values allow for meaningless clustering to occur so the points would take odd shapes or even form separate clusters. Perplexity is a parameter that is used to balance global (how points affect each other at a distance) and local (points' effects on those near them) interactions. So if set too low, the \"number of effective neighbors\" will be too low and the correct number of points will not cluster together yielding a result different from the desired one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from graphviz import Graph\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn import manifold\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "from ourStatistics import interpolate_by_mean as ibm, runPCA\n",
    "from palettable.cmocean.sequential import Dense_3\n",
    "from palettable.colorbrewer.qualitative import Set2_4\n",
    "\n",
    "#Read data and interpolate values\n",
    "cortex_data = pd.read_excel('Data_Cortex_Nuclear.xls').apply(ibm, axis=0)\n",
    "\n",
    "#Slice data and take only numeric data\n",
    "cCSs_data = cortex_data[cortex_data['class'] == 'c-SC-s'].select_dtypes(include=[np.number])\n",
    "tCSs_data = cortex_data[cortex_data['class'] == 't-SC-s'].select_dtypes(include=[np.number])\n",
    "\n",
    "#Standradize data for PCA\n",
    "cCSs_data = pd.DataFrame(StandardScaler().fit_transform(cCSs_data))\n",
    "tCSs_data = pd.DataFrame(StandardScaler().fit_transform(tCSs_data))\n",
    "\n",
    "#Run PCA\n",
    "scores_cCSs, loadings_cCSs, summary_cCSs = runPCA(cCSs_data)\n",
    "scores_tCSs, loadings_tCSs, summary_tCSs = runPCA(tCSs_data)\n",
    "\n",
    "#Turn data into ISOMAP\n",
    "def iso(std_data, num_neighbors, num_comp):\n",
    "    iso = manifold.Isomap(n_neighbors=num_neighbors, n_components=num_comp)\n",
    "    iso.fit(std_data)\n",
    "    pre_data = iso.transform(std_data)\n",
    "    col_list = [\"Component {}\".format(i) for i in range(1, num_comp+1)]\n",
    "    return pd.DataFrame(pre_data, columns=col_list)\n",
    "\n",
    "cCSs_iso = iso(cCSs_data, 10, 2)\n",
    "tCSs_iso = iso(tCSs_data, 10, 2)\n",
    "\n",
    "#Plot the 2 side-by-side for comparison\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].set_title(\"Principal Component Analysis\")\n",
    "ax[0].scatter(scores_cCSs[\"PC1\"], scores_cCSs[\"PC2\"], color='blue', label=\"control\")\n",
    "ax[0].scatter(scores_tCSs[\"PC1\"], scores_tCSs[\"PC2\"], color='red', label=\"treatment\")\n",
    "\n",
    "ax[1].set_title(\"ISOMAP\")\n",
    "ax[1].scatter(cCSs_iso['Component 1'], cCSs_iso['Component 2'], color='blue', label=\"control\")\n",
    "ax[1].scatter(tCSs_iso['Component 1'], tCSs_iso['Component 2'], color='red', label=\"treatment\")\n",
    "\n",
    "ax[0].legend(loc=\"upper left\")\n",
    "fig.savefig('dimension reduction comparison.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these two plots, I would say that ISOMAP would be a better choice since it actually shows distinct clustering. The ISOMAP would thus allow us to better find related genes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read breast cancer data and interpolate values\n",
    "bc_data = pd.read_excel('breast-cancer-wisconsin.xlsx').apply(ibm, axis=0)\n",
    "\n",
    "# Standardize dataset - except class\n",
    "class_col = bc_data.pop('class')\n",
    "bc_data_std = pd.DataFrame(StandardScaler().fit_transform(bc_data)) # Standardize\n",
    "#bc_data_std = pd.DataFrame(MinMaxScaler().fit_transform(bc_data)) # Normalize\n",
    "bc_data_std['class'] = class_col\n",
    "\n",
    "# Run t-SNE\n",
    "def tsne_reduction(pandas_data, n_comps=2, verb=1, perp=30, num_iter=500, initial=\"random\"):\n",
    "    tsne = TSNE(n_components = n_comps, verbose = verb, perplexity = perp, n_iter=num_iter, init=initial)\n",
    "    tsne_results = tsne.fit_transform(pandas_data.values)\n",
    "    col_list = [\"Component {}\".format(i) for i in range(1, n_comps+1)]\n",
    "    return pd.DataFrame(tsne_results, columns=col_list)\n",
    "\n",
    "def remove_outliers(pandas_data, within_std=3):\n",
    "    '''\n",
    "    Calculates z score for pandas dataframe and returns dataframe with entries that are within chosen deviation away\n",
    "    :param pandas_data: Pandas Dataframe of data\n",
    "    :param within_std: Number of standard deviations away from mean to accept\n",
    "    :return: Dataframe with rows removed that are outside specified range\n",
    "    '''\n",
    "    return pandas_data[(np.abs(stats.zscore(pandas_data)) < within_std).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 1 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.001s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 234.321701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 7.895314\n[t-SNE] Computing 4 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.000s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 1.432152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 94.078110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 0.417166\n[t-SNE] Computing 7 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.000s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 4.555133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 78.500938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 0.282498\n[t-SNE] Computing 10 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.001s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 11.790497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 67.260651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 0.233865\n[t-SNE] Computing 13 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.001s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 18.997268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 61.725636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 0.210263\n[t-SNE] Computing 16 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.001s...\n[t-SNE] Computed neighbors for 699 samples in 0.003s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 27.457005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 58.920761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 0.196595\n[t-SNE] Computing 1 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.001s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: -51.209171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: -7.931969\n[t-SNE] Computing 4 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.001s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 1.432152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 91.691208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 0.649064\n[t-SNE] Computing 7 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.000s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 4.555133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 103.636971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 1.339681\n[t-SNE] Computing 10 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.001s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 11.790497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 80.626549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 1.079833\n[t-SNE] Computing 13 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.001s...\n[t-SNE] Computed neighbors for 699 samples in 0.002s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 18.997268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 104.320251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 1.064245\n[t-SNE] Computing 16 nearest neighbors...\n[t-SNE] Indexed 699 samples in 0.001s...\n[t-SNE] Computed neighbors for 699 samples in 0.003s...\n[t-SNE] Computed conditional probabilities for sample 699 / 699\n[t-SNE] Mean sigma: 27.457005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 105.174507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Error after 1000 iterations: 1.020833\n"
     ]
    }
   ],
   "source": [
    "# Create lists of perplexities and initializing schemes to test\n",
    "perplexities = [5,10,20,30,40,50]\n",
    "initiate = ['random', 'pca']\n",
    "\n",
    "# Generate new pandas table for our results\n",
    "bc_data_tsne = bc_data_std.copy()\n",
    "colors = [\"blue\" if row is 2 else \"red\" for row in list(bc_data_tsne['class'])]\n",
    "labels = [\"benign\" if row is 2 else \"malignant\" for row in list(bc_data_tsne['class'])]\n",
    "bc_data_tsne['colors'] = colors\n",
    "bc_data_tsne['labels'] = labels\n",
    "\n",
    "# Iterate over perplexities and generate subplots\n",
    "for j in range(len(initiate)):\n",
    "    # Create initial figure\n",
    "    fig, ax = plt.subplots(1, len(perplexities), figsize=(30, 5))\n",
    "    legend_ele = [Line2D([0], [0], color='w', label='benign', marker='o', markerfacecolor='b'),Line2D([0], [0], color='w', label='malignant', marker='o', markerfacecolor='r')]\n",
    "    ax[0].legend(handles=legend_ele, loc='upper left')\n",
    "    fig.suptitle(\"{} initialization\".format(initiate[j]), x=0.5, y=1.0, fontsize=18)\n",
    "    for i in range(len(perplexities)):\n",
    "        tsne_comps = tsne_reduction(bc_data, perp=i, initial=initiate[j], num_iter=1000)\n",
    "        \n",
    "        # Remove outliers to allow us to see clusters/trends\n",
    "        #tsne_comps = remove_outliers(tsne_comps, within_std=3) # Uses zscore\n",
    "        tsne_comps = tsne_comps[tsne_comps.apply(lambda x: np.abs(x - x.median()) / x.std() < 3).all(axis=1)] # Median\n",
    "        \n",
    "        # Add data to our tsne dataframe\n",
    "        bc_data_tsne['{}_Perp{}_Component 1'.format(initiate[j], perplexities[i])] = tsne_comps['Component 1']\n",
    "        bc_data_tsne['{}_Perp{}_Component 2'.format(initiate[j], perplexities[i])] = tsne_comps['Component 2']\n",
    "        \n",
    "        # Create graphs\n",
    "        ax[i].set_title(\"Perplexity = {}\".format(perplexities[i]))\n",
    "        ax[i].scatter(bc_data_tsne['{}_Perp{}_Component 1'.format(initiate[j], perplexities[i])], \\\n",
    "                      bc_data_tsne['{}_Perp{}_Component 2'.format(initiate[j], perplexities[i])]\\\n",
    "                      , color=colors, label=labels)\n",
    "        ax[i].set_xlabel(\"{} Component 1\".format(initiate[j]))\n",
    "        if i == 0:\n",
    "            ax[i].set_ylabel(\"{} Component 2\".format(initiate[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both sets of graphs, the data did not nicely separate. However, when we initialize using PCA, we begin to see some separation in the lower values of perplexity. After we get past a perplexity value of 10, the separation begins to disappear. By setting th perplexity too high, the algorithm attempts to make sure each point has more neighbors than it should, and this causes the two groups to begin merging again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part a  \n",
    "  \n",
    "Using the same dataset as in assignment sheet 2, write code to fill in the missing values. Then compute the Pearson correlation between any pair of variables, and store them in a matrix. (5P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file to pandasDF\n",
    "file_path = \"breast-cancer-wisconsin.xlsx\"\n",
    "breast_df = pd.read_excel(os.path.abspath(file_path), index_col=\"code\")\n",
    "\n",
    "# Fill missing values\n",
    "benign_imputed = breast_df[breast_df[\"class\"] == 2].apply(ibm, axis=0)\n",
    "malign_imputed = breast_df[breast_df[\"class\"] == 4].apply(ibm, axis=0)\n",
    "breast_imputed_df = pd.concat([malign_imputed, benign_imputed])\n",
    "# breast_df = breast_df.apply(ibm, axis=0)\n",
    "\n",
    "# Get Pearson Correlation Matrix\n",
    "breast_corr_df = breast_df.corr(method='pearson', min_periods=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B  \n",
    "  \n",
    "Install the Graphviz library and its Python interface. Generate and visualize some simple graph. You can find the software and its documentation at https://pypi.python.org/pypi/graphviz. (5P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vclsi-3-Schultz-Ibarra-simple_graph.gv.pdf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph(\"Simple Graph\", filename=\"vclsi-3-Schultz-Ibarra-simple_graph.gv\")\n",
    "g.node(\"Me\")\n",
    "g.edge(\"Me\", \"The girl I like\")\n",
    "g.edge(\"Her boyfriend\", \"The girl I like\")\n",
    "g.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part C  \n",
    "  \n",
    "Create a graph from the correlation matrix and visualize it. Represent each variable as a node in the graph. Insert an edge between two variables whenever the Pearson correlation between them exceeds the threshold ρ > 0.6. (4P)\n",
    "\n",
    "Part D\n",
    "\n",
    "Modify the visual attributes of edges to reflect the magnitude of the correlation. (3P)\n",
    "\n",
    "Part E\n",
    "\n",
    "Produce an alternative visualization with a circular layout. Color the nodes so that there are four set of nodes, one color for having at least one correlation more than 0.9 to other nodes, another for having at least a correlation 0.8 < ρ max <= 0.9, one for having a correlation 0.6 < ρ max <= 0.8 and the last for the remaining nodes. (5P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vclsi-3-Schultz-Ibarra-breast_graph.gv.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correlation graph\n",
    "breast_g = Graph(\"Breast Cancer\", filename=\"vclsi-3-Schultz-Ibarra-breast_graph.gv\", engine=\"circo\")\n",
    "\n",
    "# Removing diagonal from correlation matrix\n",
    "breast_corr_df_ndiag = (breast_corr_df[breast_corr_df < 1])\n",
    "column_max = breast_corr_df_ndiag.max()\n",
    "\n",
    "\n",
    "def select_color(val):\n",
    "    \"\"\"\n",
    "    selects a color from a value\n",
    "    :param val: value to which select color\n",
    "    :return: color for the value\n",
    "    \"\"\"\n",
    "    if val >= 0.9:\n",
    "        return Set2_4.hex_colors[0]\n",
    "    elif val >= 0.8:\n",
    "        return Set2_4.hex_colors[1]\n",
    "    elif val >= 0.6:\n",
    "        return Set2_4.hex_colors[2]\n",
    "    else:\n",
    "        return Set2_4.hex_colors[3]\n",
    "\n",
    "\n",
    "# Creating nodes in the graph\n",
    "for c in breast_corr_df.columns.tolist():\n",
    "    breast_g.node(c, color=select_color(column_max[c]), style=\"filled\", fillcolor=select_color(column_max[c]))\n",
    "\n",
    "# Get boolean matrix for values greater than the threshold.\n",
    "threshold = 0.6\n",
    "breast_corr_bool = breast_corr_df[breast_corr_df <= threshold].isnull()\n",
    "\n",
    "# Get color map to use\n",
    "cmap = Dense_3.mpl_colormap\n",
    "\n",
    "# Check if correlation coefficient covers the threshold if it does add edge\n",
    "for i, j in combinations(breast_corr_df.columns.tolist(), r=2):\n",
    "    if breast_corr_bool.loc[i, j]:\n",
    "        val = breast_corr_df.loc[i, j]\n",
    "        breast_g.edge(i, j, color=matplotlib.colors.to_hex(cmap(val)), penwidth=str((val*11)-5))\n",
    "\n",
    "breast_g.view()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the selected threshold, which nodes are disconnected from the rest of the graph and what do they indicate? (1P)\n",
    "\n",
    "Only the node \"mitoses\" is disconnected, it indicates it has a weak correlation with the rest of the variables of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two nodes A and B are strongly correlated, and node C is strongly correlated with node B, can we conclude that node C will be also strongly correlated with node A? (1P)\n",
    "\n",
    "Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where have you already seen the four nodes connected to node “class” through its thickest edges? (1P)  \n",
    "\n",
    "BlahCrhoma  \n",
    "uniCellS  \n",
    "uniCellShape  \n",
    "bareNuc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
